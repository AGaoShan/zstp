import json
import os
import traceback
from concurrent.futures import ThreadPoolExecutor, as_completed
from typing import Dict, Tuple, List, Any
import asyncio


from input.md import chunk_md_with_headers
from llm.invoker import extract_audit_insights


def process_single_report(file_path: str, output_dir: str) -> Tuple[str, str]:
    """
    å¤„ç†å•ä¸ªå®¡è®¡æŠ¥å‘Šæ–‡ä»¶ï¼Œå¹¶åœ¨åŒæ­¥çº¿ç¨‹ä¸­è¿è¡Œå¼‚æ­¥å‡½æ•°ã€‚
    """
    try:
        file_ext = os.path.splitext(file_path)[1].lower()
        extracted_knowledge: List[Dict[str, Any]] = []

        # --- 1. å¼‚æ­¥åˆ†å—å’Œæå–é€»è¾‘ ---

        # å®šä¹‰ä¸€ä¸ªå†…éƒ¨çš„å¼‚æ­¥å‡½æ•°æ¥å°è£…å¼‚æ­¥è°ƒç”¨é“¾
        async def run_async_tasks():
            chunks: List[str] = []

            if file_ext == '.md':
                # ä¼ å…¥æ–‡ä»¶è·¯å¾„ç»™å¼‚æ­¥åˆ†å—å‡½æ•°ï¼Œå®ƒåœ¨å†…éƒ¨å¤„ç†è¯»å–å’Œåˆ†å—
                # æ³¨æ„ï¼šè¿™é‡Œè°ƒç”¨çš„æ˜¯ await chunk_md_with_headers(file_path)
                chunks = await chunk_md_with_headers(file_path,headers_to_split_on= [("##", "Header 2")])

            elif file_ext == '.txt':
                # å¯¹äº txt æ–‡ä»¶ï¼Œä»éœ€è‡ªå·±åŒæ­¥è¯»å–å†…å®¹ï¼Œç„¶åä½œä¸ºå•ä¸ªå—å¤„ç†
                try:
                    with open(file_path, 'r', encoding='utf-8') as f:
                        content = f.read()
                    chunks = [content]
                except IOError as e:
                    # å¦‚æœtxtæ–‡ä»¶è¯»å–å¤±è´¥ï¼Œç«‹å³æŠ›å‡ºé”™è¯¯
                    raise IOError(f"Failed to read TXT file: {e}")

            else:
                return []  # ä¸æ”¯æŒçš„æ–‡ä»¶ç±»å‹ï¼Œè™½ç„¶åœ¨å¤–å±‚å·²è¿‡æ»¤

            insights = []
            for chunk in chunks:
                # è°ƒç”¨ LLM æå–æ´å¯Ÿï¼ˆå‡è®¾ extract_audit_insights ä¹Ÿæ˜¯ async defï¼‰
                insight = extract_audit_insights(chunk)
                if insight:
                    insights.append(insight)
            return insights

        # åœ¨å½“å‰åŒæ­¥çº¿ç¨‹ä¸­ï¼Œå¯åŠ¨ä¸€ä¸ªä¸´æ—¶çš„äº‹ä»¶å¾ªç¯æ¥è¿è¡Œæ‰€æœ‰å¼‚æ­¥ä»»åŠ¡
        # è¿™å°±æ˜¯å°†å¼‚æ­¥ä»£ç å®‰å…¨åœ°è¿è¡Œåœ¨ ThreadPoolExecutor çº¿ç¨‹ä¸­çš„å…³é”®
        extracted_knowledge = asyncio.run(run_async_tasks())

        # --- 2. ç»“æœä¿å­˜ (ä¿æŒåŒæ­¥) ---
        file_name = os.path.basename(file_path)
        output_file = os.path.join(output_dir, f"{os.path.splitext(file_name)[0]}_result.json")

        with open(output_file, 'w', encoding='utf-8') as f:
            json.dump(extracted_knowledge, f, ensure_ascii=False, indent=2)

        print(f"âœ… æˆåŠŸå¤„ç†: {file_path}. æå–äº† {len(extracted_knowledge)} æ¡æ´å¯Ÿã€‚")
        return file_path, "success"

    except Exception as e:
        # æ•è·æ‰€æœ‰å¯èƒ½çš„å¼‚å¸¸ï¼ŒåŒ…æ‹¬ç”± asyncio.run å†…éƒ¨æŠ›å‡ºçš„å¼‚å¸¸
        error_msg = f"error: {type(e).__name__}: {str(e)}"
        traceback.print_exc()
        print(f"âŒ å¤„ç†å¤±è´¥: {file_path}, é”™è¯¯: {error_msg}")
        return file_path, error_msg



def batch_process_audit_reports(input_dir: str, output_dir: str, max_workers: int = 4) -> Dict[str, str]:
    """
    æ‰¹é‡å¹¶è¡Œå¤„ç†å®¡è®¡æŠ¥å‘Š
    """
    # ... (ä»£ç ä¸æ‚¨æä¾›çš„å®Œå…¨ä¸€è‡´) ...
    os.makedirs(output_dir, exist_ok=True)

    # è·å–æ‰€æœ‰æŠ¥å‘Šæ–‡ä»¶ï¼Œç°åœ¨æ”¯æŒ .txt å’Œ .md
    report_files = []
    for file in os.listdir(input_dir):
        # æ³¨æ„: å¦‚æœæ–‡ä»¶è·¯å¾„æ˜¯ç›¸å¯¹çš„ (å¦‚æ‚¨ç¤ºä¾‹ä¸­çš„ "../audit_reports")ï¼Œ
        # ç¡®ä¿ file æ˜¯å®Œæ•´è·¯å¾„ï¼Œå¦åˆ™å†…éƒ¨çš„ open/chunk_md_with_headers å¯èƒ½ä¼šå¤±è´¥ã€‚
        # æ‚¨çš„ä»£ç ä¸­ä½¿ç”¨äº† os.path.join(input_dir, file)ï¼Œè¿™æ˜¯æ­£ç¡®çš„åšæ³•ã€‚
        if file.endswith(('.txt', '.md')):
            report_files.append(os.path.join(input_dir, file))

    if not report_files:
        print("âš ï¸ æœªæ‰¾åˆ°ä»»ä½•å®¡è®¡æŠ¥å‘Šæ–‡ä»¶")
        return {}

    # å¹¶è¡Œå¤„ç†æ–‡ä»¶
    results = {}
    with ThreadPoolExecutor(max_workers=max_workers) as executor:
        # æäº¤æ‰€æœ‰ä»»åŠ¡
        future_to_file = {
            executor.submit(process_single_report, file, output_dir): file
            for file in report_files
        }

        # è·å–ç»“æœ
        for future in as_completed(future_to_file):
            file_path, status = future.result()
            results[file_path] = status

    # ç”Ÿæˆæ±‡æ€»æŠ¥å‘Š
    summary = {
        "total": len(results),
        "success": sum(1 for v in results.values() if v == "success"),
        "failed": sum(1 for v in results.values() if v.startswith("error:"))
    }

    # ä¿å­˜æ±‡æ€»ç»“æœ
    try:
        with open(os.path.join(output_dir, "summary.json"), 'w', encoding='utf-8') as f:
            json.dump(summary, f, ensure_ascii=False, indent=2)
    except Exception as e:
        print(f"âŒ ä¿å­˜æ±‡æ€»æŠ¥å‘Šå¤±è´¥: {e}")

    print(f"\nğŸ“Š å¤„ç†å®Œæˆ: æ€»è®¡ {summary['total']} ä¸ªæ–‡ä»¶, æˆåŠŸ {summary['success']} ä¸ª, å¤±è´¥ {summary['failed']} ä¸ª")
    return results


# # ä½¿ç”¨ç¤ºä¾‹
# if __name__ == "__main__":
#     # ... (ä»£ç ä¸æ‚¨æä¾›çš„å®Œå…¨ä¸€è‡´) ...
#     INPUT_DIRECTORY = "./audit_reports"  # å­˜æ”¾å®¡è®¡æŠ¥å‘Šçš„ç›®å½•
#     OUTPUT_DIRECTORY = "./extracted_knowledge"  # è¾“å‡ºç»“æœçš„ç›®å½•
#
#
#     # æ‰§è¡Œæ‰¹é‡å¤„ç†
#     results = batch_process_audit_reports(
#         input_dir=INPUT_DIRECTORY,
#         output_dir=OUTPUT_DIRECTORY,
#         max_workers=8  # æ ¹æ®CPUæ ¸å¿ƒæ•°è°ƒæ•´
#     )